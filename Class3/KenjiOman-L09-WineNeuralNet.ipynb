{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9 Assignment - Wine Neural Network\n",
    "\n",
    "## Author - Kenji Oman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "For this assignment you will start from the perceptron neural network notebook (Simple Perceptron Neural Network.ipynb) and modify the python code to make it into a multi-layer neural network. To test your system, use the RedWhiteWine.csv file with the goal of building a red or white wine classifier. Use all the features in the dataset, allowing the network to decide how to build the internal weighting system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1. Use the provided RedWhiteWine.csv file. Include ALL the features with “Class” being your output vector\n",
    "2. Use the provided Simple Perceptron Neural Network notebook (copied below) to develop a multi-layer feed-forward/backpropagation neural network\n",
    "4. Be able to adjust the following between experiments:\n",
    "<ul>\n",
    "<li>Learning Rate\n",
    "<li>Number of epochs\n",
    "<li>Depth of architecture—number of hidden layers between the input and output layers\n",
    "<li>Number of nodes in a hidden layer—width of the hidden layers\n",
    "<li>(optional) Momentum\n",
    "    </ul>\n",
    "5. Determine what the best neural network structure and hyperparameter settings results in the\n",
    "best predictive capability\n",
    "\n",
    "# Import data and do a train/ validate/ test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Set\n",
    "#URL = \"https://library.startlearninglabs.uw.edu/DATASCI420/Datasets/RedWhiteWine.csv\"\n",
    "URL = \"RedWhiteWine.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  Class  \n",
       "0      9.4        5      1  \n",
       "1      9.8        5      1  \n",
       "2      9.8        5      1  \n",
       "3      9.8        6      1  \n",
       "4      9.4        5      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data, and take a look\n",
    "df = pd.read_csv(URL)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4417, 13), (1105, 13), (975, 13))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, split into 3\n",
    "temp_df, test_df = train_test_split(df, test_size=0.15, stratify=df.Class, random_state=0)\n",
    "train_df, validate_df = train_test_split(temp_df, test_size=0.2, stratify=temp_df.Class, random_state=1)\n",
    "train_df.shape, validate_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks like it is in shape for running a perceptron model. Let's now define our perceptron class.\n",
    "\n",
    "# Perceptron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronNN(object):\n",
    "    def __init__(self, learning_rate=0.01, epochs=100, depth=0, hidden_width=12):\n",
    "        \"\"\"\n",
    "        learning_rate {float} = the learning rate, default=0.01\n",
    "        epochs {int} = the number of training epochs, default=100\n",
    "        depth {int} = the number of hidden layers, default=0\n",
    "        hidden_width {int} = number of nodes within a hidden layer, default=12\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set the internal parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.depth = depth\n",
    "        self.hidden_width = hidden_width\n",
    "        \n",
    "    # Define the sigmoid function\n",
    "    def sigmoid(self, x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        if x.any()>=0:\n",
    "            return 1/(1 + np.exp(-x))\n",
    "        else:\n",
    "            return np.exp(x)/(1 + np.exp(x))\n",
    "        \n",
    "    # Initialize parameters for the network\n",
    "    def init_parameters(self, n_layers, n_obs, std=1e-1, random=True):\n",
    "        if(random):\n",
    "            return(np.random.random([n_layers, n_obs])*std)\n",
    "        else:\n",
    "            return(np.zeros([n_layers, n_obs]))\n",
    "        \n",
    "    # Define forward propogation for one layer\n",
    "    def one_fwd_prop(self, W1, bias, X):\n",
    "        Z1 = np.dot(W1,X) + bias # dot product of the weights and X + bias\n",
    "        A1 = self.sigmoid(Z1)  # Uses sigmoid to create a predicted vector\n",
    "\n",
    "        return(A1)\n",
    "    \n",
    "    def full_fwd_prop(self, W_l, B_l, X):\n",
    "        \"\"\"\n",
    "        Function to run through all the forward propagation steps for a given epoch.\n",
    "        \n",
    "        Args:\n",
    "        W_l = list of weight vectors\n",
    "        B_l = list of bias vectors\n",
    "        X = input vector\n",
    "        \"\"\"\n",
    "        # Set the list of activations, using the input vector as the first\n",
    "        # input to forward prop\n",
    "        A_l = [self.one_fwd_prop(W_l[0], B_l[0], X)]\n",
    "\n",
    "        # And for each of the hidden layers\n",
    "        for layer in range(self.depth):\n",
    "            # Calculate its activation\n",
    "            A_l.append(self.one_fwd_prop(W_l[layer+1], B_l[layer+1], A_l[-1]))\n",
    "            \n",
    "        return A_l\n",
    "    \n",
    "    # And, back propagation for one layer\n",
    "    def one_back_prop(self, A1, W1, bias, X, Y):\n",
    "        m = np.shape(X)[1] # used the calculate the cost by the number of inputs -1/m\n",
    "\n",
    "        # Cross entropy loss function\n",
    "        cost = (-1/m)*np.sum(Y*np.log(A1) + (1-Y)*np.log(1-A1)) # cost of error\n",
    "        dZ1 = A1 - Y                                            # subtract actual from pred weights\n",
    "        dW1 = (1/m) * np.dot(dZ1, X.T)                          # calc new weight vector\n",
    "        dBias = (1/m) * np.sum(dZ1, axis = 1, keepdims = True)  # calc new bias vector\n",
    "\n",
    "        grads ={\"dW1\": dW1, \"dB1\":dBias} # Weight and bias vectors after backprop\n",
    "\n",
    "        return(grads,cost)\n",
    "    \n",
    "    def full_back_prop(self, A_l, W_l, B_l, X, Y):\n",
    "        \"\"\"Run through all the back propogation steps, calculating\n",
    "        the new set of weights/ biases\n",
    "        \n",
    "        Args:\n",
    "        A_l = list of activation layers\n",
    "        W_l = list of weights\n",
    "        B_l = list of biases\n",
    "        X = input matrix\n",
    "        Y = prediction target\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, do for the final layer\n",
    "        # If we have hidden layers\n",
    "        if self.depth != 0:\n",
    "            grads, cost = self.one_back_prop(A_l[-1], W_l[-1], B_l[-1], A_l[-2], Y)\n",
    "            W_l[-1] -= self.learning_rate*grads[\"dW1\"]    # update weight vector LR*gradient*[BP weights]\n",
    "            B_l[-1] -= self.learning_rate*grads[\"dB1\"]    # update bias LR*gradient[BP bias]\n",
    "\n",
    "            # Now, do for remaining layers\n",
    "            for layer in range(self.depth -2, 0, -1):\n",
    "                grads, temp_cost = self.one_back_prop(A_l[layer], W_l[layer], B_l[layer], A_l[layer - 1], A_l[layer])\n",
    "                W_l[layer] -= self.learning_rate*grads[\"dW1\"]    # update weight vector LR*gradient*[BP weights]\n",
    "                B_l[layer] -= self.learning_rate*grads[\"dB1\"]    # update bias LR*gradient[BP bias]\n",
    "\n",
    "            # And, do for the very first layer\n",
    "            grads, temp_cost = self.one_back_prop(A_l[0], W_l[0], B_l[0], X, A_l[1])\n",
    "        else:\n",
    "            # If we don't have any layers\n",
    "            grads, cost = self.one_back_prop(A_l[-1], W_l[-1], B_l[-1], X, Y)\n",
    "        W_l[0] -= self.learning_rate*grads[\"dW1\"]    # update weight vector LR*gradient*[BP weights]\n",
    "        B_l[0] -= self.learning_rate*grads[\"dB1\"]    # update bias LR*gradient[BP bias]\n",
    "        \n",
    "        return W_l, B_l, cost\n",
    "        \n",
    "    \n",
    "    # And finally, gradient descent to run it all\n",
    "    def run_grad_desc(self, X, Y):\n",
    "        \n",
    "        # To make reproducible, set random seed\n",
    "        np.random.seed(12345)\n",
    "        \n",
    "        # Transpose the X/ Y to make them consistent with the rest of the example\n",
    "        # code.\n",
    "        X = X.T\n",
    "        Y = Y.T\n",
    "        \n",
    "        # Grab the dimensionality of the X vector (transposed, so\n",
    "        # we have (num_features, num_observations))\n",
    "        n_features, m_obs = np.shape(X)\n",
    "\n",
    "        # Initialize weights for each initial layer\n",
    "        # If we don't have any hidden layers\n",
    "        if self.depth == 0:\n",
    "            W1 = self.init_parameters(1, n_features, True)\n",
    "            B1 = self.init_parameters(1, 1, True)\n",
    "            W_h = []\n",
    "            B_h = []\n",
    "        else:\n",
    "            # Otherwise, we need to initialize going to the hidden layers\n",
    "            W1 = self.init_parameters(self.hidden_width, n_features, True)\n",
    "            B1 = self.init_parameters(self.hidden_width, 1, True)\n",
    "        \n",
    "            # And the parameters for each hidden layer\n",
    "            W_h = [self.init_parameters(self.hidden_width, self.hidden_width, True) for i in range(0, self.depth - 1)]\n",
    "            B_h = [self.init_parameters(self.hidden_width, 1, True) for i in range(0, self.depth - 1)]\n",
    "            \n",
    "            # And, exiting the hidden layer to make the prediction\n",
    "            W_h.append(self.init_parameters(1, self.hidden_width, True))\n",
    "            B_h.append(self.init_parameters(1, 1, True))\n",
    "        \n",
    "        # Combine these two into one list to make it easier to manage\n",
    "        W_l = [W1] + W_h\n",
    "        B_l = [B1] + B_h\n",
    "\n",
    "        loss_array = np.ones([self.epochs])*np.nan # resets the loss_array to NaNs\n",
    "\n",
    "        for i in np.arange(self.epochs):\n",
    "            \n",
    "            # Go through all the forward propogations\n",
    "            A_l = self.full_fwd_prop(W_l, B_l, X)\n",
    "            \n",
    "            # Now that we calculated the activation layers, let's do\n",
    "            # back propagation\n",
    "            W_l, B_l, cost = self.full_back_prop(A_l, W_l, B_l, X, Y)\n",
    "\n",
    "            # also, store the loss we had from this epoch\n",
    "            loss_array[i] = cost                    # loss array gets cross ent values\n",
    "\n",
    "        # set the parameters (weights/ biases we calculated)\n",
    "        parameter = {\"W_l\": W_l, \"B_l\": B_l}\n",
    "\n",
    "        return(parameter,loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try running the model once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koman/miniconda3/envs/dsc/lib/python3.6/site-packages/ipykernel_launcher.py:63: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24615384615384617"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model\n",
    "pp_nn = PerceptronNN(depth=2, hidden_width=1)\n",
    "\n",
    "# Run it\n",
    "params, loss = pp_nn.run_grad_desc(train_df.drop(columns='Class').values, train_df.Class.values)\n",
    "\n",
    "# And test it\n",
    "temp = pp_nn.full_fwd_prop(params['W_l'], params['B_l'], validate_df.drop(columns='Class').T)\n",
    "((pd.Series(temp[-1][0]) > 0.5).astype(int) == validate_df.Class.reset_index(drop=True)).sum() / validate_df.Class.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks like with just a quick run, we got an accuracy of 24.6%.  Now, let's try searching for an optimum set of parameters.\n",
    "\n",
    "# Gridsearch through hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'learning_rate': 0.001, 'epochs': 100, 'depth': 0},\n",
       " {'learning_rate': 0.001, 'epochs': 100, 'depth': 1, 'hidden_width': 1},\n",
       " {'learning_rate': 0.001, 'epochs': 100, 'depth': 1, 'hidden_width': 3},\n",
       " {'learning_rate': 0.001, 'epochs': 100, 'depth': 1, 'hidden_width': 6},\n",
       " {'learning_rate': 0.001, 'epochs': 100, 'depth': 1, 'hidden_width': 9}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a list of sets of parameters we want to try\n",
    "input_params = []\n",
    "# Now, set up the kinds of parameter values we want to try\n",
    "for learn in [0.001, 0.01, 0.1]:\n",
    "    for epoch in [100, 500]:#, 1_000]:\n",
    "        for depth in [0, 1, 2, 3]:#, 4, 5]:\n",
    "            if depth != 0:\n",
    "                for width in [1, 3, 6, 9, 12, 15]:\n",
    "                    input_params.append({\n",
    "                        'learning_rate': learn,\n",
    "                        'epochs': epoch,\n",
    "                        'depth': depth,\n",
    "                        'hidden_width': width\n",
    "                    })\n",
    "            else:\n",
    "                input_params.append({\n",
    "                    'learning_rate': learn,\n",
    "                    'epochs': epoch,\n",
    "                    'depth': depth\n",
    "                })\n",
    "input_params[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koman/miniconda3/envs/dsc/lib/python3.6/site-packages/ipykernel_launcher.py:63: RuntimeWarning: divide by zero encountered in log\n",
      "/home/koman/miniconda3/envs/dsc/lib/python3.6/site-packages/ipykernel_launcher.py:63: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>weights</th>\n",
       "      <th>loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'learning_rate': 0.001, 'epochs': 100, 'depth...</td>\n",
       "      <td>{'W_l': [[[ 0.85398276  0.31417314  0.17994224...</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan, nan, nan, nan, ...</td>\n",
       "      <td>0.905882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'learning_rate': 0.001, 'epochs': 100, 'depth...</td>\n",
       "      <td>{'W_l': [[[ 0.77619612  0.30865069  0.17785691...</td>\n",
       "      <td>[0.8163957621617163, 0.8161454319446956, 0.815...</td>\n",
       "      <td>0.246154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'learning_rate': 0.001, 'epochs': 100, 'depth...</td>\n",
       "      <td>{'W_l': [[[0.88429571 0.31424893 0.1819035  0....</td>\n",
       "      <td>[2.209779137636132, 2.2078216554054824, 2.2058...</td>\n",
       "      <td>0.246154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'learning_rate': 0.001, 'epochs': 100, 'depth...</td>\n",
       "      <td>{'W_l': [[[0.91609926 0.31574088 0.18331833 0....</td>\n",
       "      <td>[3.2032184057435296, 3.199389984160236, 3.1955...</td>\n",
       "      <td>0.246154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'learning_rate': 0.001, 'epochs': 100, 'depth...</td>\n",
       "      <td>{'W_l': [[[0.9261892  0.31621465 0.18376666 0....</td>\n",
       "      <td>[4.330096401690009, 4.324461007454486, 4.31882...</td>\n",
       "      <td>0.246154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     hyperparameters  \\\n",
       "0  {'learning_rate': 0.001, 'epochs': 100, 'depth...   \n",
       "1  {'learning_rate': 0.001, 'epochs': 100, 'depth...   \n",
       "2  {'learning_rate': 0.001, 'epochs': 100, 'depth...   \n",
       "3  {'learning_rate': 0.001, 'epochs': 100, 'depth...   \n",
       "4  {'learning_rate': 0.001, 'epochs': 100, 'depth...   \n",
       "\n",
       "                                             weights  \\\n",
       "0  {'W_l': [[[ 0.85398276  0.31417314  0.17994224...   \n",
       "1  {'W_l': [[[ 0.77619612  0.30865069  0.17785691...   \n",
       "2  {'W_l': [[[0.88429571 0.31424893 0.1819035  0....   \n",
       "3  {'W_l': [[[0.91609926 0.31574088 0.18331833 0....   \n",
       "4  {'W_l': [[[0.9261892  0.31621465 0.18376666 0....   \n",
       "\n",
       "                                                loss  validation_accuracy  \n",
       "0  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...             0.905882  \n",
       "1  [0.8163957621617163, 0.8161454319446956, 0.815...             0.246154  \n",
       "2  [2.209779137636132, 2.2078216554054824, 2.2058...             0.246154  \n",
       "3  [3.2032184057435296, 3.199389984160236, 3.1955...             0.246154  \n",
       "4  [4.330096401690009, 4.324461007454486, 4.31882...             0.246154  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, run our gridsearch, and store the accuracy values\n",
    "results = {'hyperparameters': [], 'weights': [], 'loss': [], 'validation_accuracy': []}\n",
    "for settings in input_params:\n",
    "    # Store the hyperparameters\n",
    "    results['hyperparameters'].append(settings)\n",
    "    \n",
    "    # Define our model\n",
    "    pp_nn = PerceptronNN(**settings)\n",
    "\n",
    "    # Run it\n",
    "    weights, loss = pp_nn.run_grad_desc(train_df.drop(columns='Class').values, train_df.Class.values)\n",
    "    \n",
    "    # Store the parameters and loss\n",
    "    results['weights'].append(weights)\n",
    "    results['loss'].append(loss)\n",
    "\n",
    "    # And test it\n",
    "    temp = pp_nn.full_fwd_prop(weights['W_l'], weights['B_l'], validate_df.drop(columns='Class').T)\n",
    "    \n",
    "    # And store the accuracy\n",
    "    results['validation_accuracy'].append(((pd.Series(temp[-1][0]) > 0.5).astype(int) == validate_df.Class.reset_index(drop=True)).sum() / validate_df.Class.shape[0])\n",
    "    \n",
    "# Once done running everything, let's make into a dataframe, so we can quickly find the best one\n",
    "results = pd.DataFrame(results)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>weights</th>\n",
       "      <th>loss</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>{'learning_rate': 0.01, 'epochs': 500, 'depth'...</td>\n",
       "      <td>{'W_l': [[[ 0.61793031  0.36127919  0.14693383...</td>\n",
       "      <td>[nan, nan, 7.583493738988932, 6.27357264387054...</td>\n",
       "      <td>0.949321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      hyperparameters  \\\n",
       "57  {'learning_rate': 0.01, 'epochs': 500, 'depth'...   \n",
       "\n",
       "                                              weights  \\\n",
       "57  {'W_l': [[[ 0.61793031  0.36127919  0.14693383...   \n",
       "\n",
       "                                                 loss  validation_accuracy  \n",
       "57  [nan, nan, 7.583493738988932, 6.27357264387054...             0.949321  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, let's see which set of hyper parameters gave us the highest accuracy\n",
    "results[results.validation_accuracy == results.validation_accuracy.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'learning_rate': 0.01, 'epochs': 500, 'depth': 0}], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can't see the hyperparameters, so show that explicitly\n",
    "results.loc[results.validation_accuracy == results.validation_accuracy.max(), 'hyperparameters'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks like our best performing model used a learning rate of 0.1, trained for 500 epochs, and had no hidden layers.  Let's try testing this model on our test data to see what accuracy we get there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9425641025641026"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final test with best performing hyperparameters\n",
    "# First, grab the line (includes the weights)\n",
    "best = results[results.validation_accuracy == results.validation_accuracy.max()]\n",
    "# Define the model object (to allow us to test the best model)\n",
    "pp_nn = PerceptronNN(**best.hyperparameters.values[0])\n",
    "\n",
    "# And test it\n",
    "temp = pp_nn.full_fwd_prop(best.weights.values[0]['W_l'], best.weights.values[0]['B_l'], test_df.drop(columns='Class').T)\n",
    "((pd.Series(temp[-1][0]) > 0.5).astype(int) == test_df.Class.reset_index(drop=True)).sum() / test_df.Class.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we observe that our test accuracy 94.3%, so pretty good!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
